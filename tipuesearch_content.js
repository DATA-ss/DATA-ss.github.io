var tipuesearch = {"pages":[{"title":"About","text":"Who am I? I'm an engineer, data science enthusiast and sports fan that just started his blog, check it out!","tags":"blog","url":"about/","loc":"about/"},{"title":"Takeaways from Amazon's Machine Learning Use Case: Call Center","text":"Home of Big Data - Amazon I recently completed Amazon's short machine learning Use Case that was centered on improving the efficiency and accuracy of their call center routing system. Here are a few takeways I took note of from the course. 1.0 Problem Framing Before embarking on implementing a machine learning solution for your business, you should first frame the problem within the context of your business. A simple guide would be to obey the following steps: Understand context Identify baseline Convert business problem -> ML problem When it comes to understanding context , you should be prepared to consult with domain experts if necessary. Be sure to ask them wide ranging questions so that you could properly frame the business problem and account for edge cases within a reasonable and solvable context. Meanwhile it is also important to identify a baseline as it allows Opportunity Analysis to be performed. With a baseline, performance can be compared. If there's no production level decision making system in place, use an educated guess. Once these measures have been taken, you would be better prepared to convert your business problem into a machine learning one. At this stage it is crucial to think critcally about how a correct prediction could change your business metric. 2.0 Data Handling After the previous stage is completed, you should start looking at your data and ascertain whether you have what you need to attempt a machine learning solution. This is the most important stage of the entire process, and is where most of the mistakes can occur. Of course its not that simple; you also have to look at other aspects of your data and see if you can simplify it where logically appropriate. A good rule of thumb to follow is to check if there are labels in minor quantities that are similar enough to be combined; the fewer labels the better. 2.1 Train/Development/Test Compared to what I've learnt which is a regular train-test split, Amazon suggests a three part split of your data when developing the machine learning model. Doing this can better help with overfitting. Amazon has suggested a 70-15-15 split if you have a lot of data available. Otherwise do a 80-10-10 split. 3.0 Modeling Amazon prioritizes the Precision-Recall Tradeoff in their machine learning classification model. Alongside that they would perform Feature Engineering , by manually looking at examples of false-positives and false-negatives to find out why the model failed. It would help if there is a domain expert was present, to help identify what kinds of info can help correct these errors. 4.0 Application Once your model is tested and ready, Amazon suggests using A/B Testing to see if the \"treatment group\" sees an improvement in performance compared to the control group. In some cases you may need add additional layers of rules to guide your algorithm's behavior in case there are certain business requirements that need to be met that the model didn't pick up in its' learning stage. Conclusion Hopefully there are some nice learnings you can takeaway from this short article, and spare you the 40 minutes 1 . I know I'm definitely going to try using the triple split of data in my models moving forward. 1 There is actually no harm spending 40 minutes going through the course if you'd like, as they'll send you a sweet little completion certificate!","tags":"blog","url":"blog/2020/02/20/takeaways-from-amazons-machine-learning-use-case-call-center/","loc":"blog/2020/02/20/takeaways-from-amazons-machine-learning-use-case-call-center/"},{"title":"How to Run Your NLTK Pythonic App on Heroku","text":"If you're having issues running your NLTK -based Pythonic app on Heroku, this guide is for you. Context For my final project at GA , I was creating an NLTK -based keyword assessment tool that parses user-provided job descriptions and returns the relevant keywords that the user should prioritize on their resume. * key package: NLTK Additionally, I have a companion Word Cloud component that shares the same text preprocessing function as the above. It involves searching/scraping job descriptions for a user-input job title, passing that corpus through the preprocessing function before it is fed to the WordCloud generator. * key packages: Selenium with Chromedriver (for webscraping) Problem If you were to push the package as is to Heroku, the app will not run. That's because it is missing some core Python dependencies. Solution To get around this, we need to install buildpacks to the Heroku app first! The disadvantage to this approach is that buildpacks can take up a lot of space. Bear in mind that free Heroku only provides you with 500MB to play with. Thus I would suggest that you avoid using buildpacks if you can help it, especially if your app may be computationally/memory intensive. NLTK Buildpack Step 1 : Before you start, you ought to have had the new app created on Heroku first. Once you have done so, navigate to \"Settings\" and scroll down to Buildpacks . Click on \"Add buildpack\" and paste the following: heroku/python Click \"Save changes\". Step 2: touch runtime.txt echo \"python-3.7.3\" >> runtime.txt What this does is create a file that specifies the Python version you want to run on your Heroku app. Feel free to change 3.7.3 to whichever Python version you desire. Step 3 : Next you'll need to create a nltk.txt file and populate it. You can do it the old-fashioned way, or create and populate it from the terminal (make sure your terminal is pointing to the app's directory). touch nltk.txt echo \"wordnet\" >> nltk.txt In my case I require the wordnet and stopwords packages from NLTK to be installed, so I'll do this instead: touch nltk.txt echo -e \"wordnet \\\\nstopwords\" >> nltk.txt Selenium/Chromedriver Buildpack Just as in Step 1 in the NLTK section above, paste and add the following to your Buildpacks. https://github.com/heroku/heroku-buildpack-google-chrome https://github.com/heroku/heroku-buildpack-chromedriver Bonus: Auto requirements.txt Generator To ensure you get the correct versions of packages used in your app, and just in case you forgot to run a venv , pipreqs is your friend. Install: pip install pipreqs Usage: From the terminal, change directory to point to your app folder where your app.py is located. Then just key in the following: pipreqs You should see a new requirements.txt file generated automatically!","tags":"blog","url":"blog/2019/12/01/how-to-run-your-nltk-pythonic-app-on-heroku/","loc":"blog/2019/12/01/how-to-run-your-nltk-pythonic-app-on-heroku/"},{"title":"Point Forward: The Job Description Hacker","text":"As the DSI course winds to a close, I've set out to produce a NLP -based model that should \"hack\" a job description and returns the top most words one should use in their job search efforts (in this case me!). There would be two parts to this project before it is uploaded to Heroku: 1) Scraping 2) Modeling Scraping To gather the necessary data, I've decided to scrape the job directories at Indeed and Linkedin. The packages necessary to achieve this are gazpacho , BeautifulSoup and Selenium . A challenge I faced in scraping them is that these websites do not have clearly defined templates for the postings; the <div> tags are unorganized/unnamed and as such its difficult to remove the fluff text that usually accompanies the job description and qualifications that are being sought. One workaround is to scrape only the <li> list tags instead, as they usually contain the proverbial â€˜meat' that we want to have on our resumes. To that end, the repo will have two versions of the scraping script: one to scrape everything under a specified <div> tag, and the other to scrape only <li> tags. Modeling To model this adequately, spaCy is the most suitable package to support this project. The idea is to have a front-end that accepts a user-input (via copy-paste) of a job description they found online, and the model would parse it for the most relevant texts that the user should use in their resume to target that specific job. STAY TUNED !!","tags":"blog","url":"blog/2019/11/12/point-forward-the-job-description-hacker/","loc":"blog/2019/11/12/point-forward-the-job-description-hacker/"},{"title":"DSI 5 Hackathon - The (YouTube) Virality Prophet","text":"As a Data Science Immersive student, I had the privilege of participating in my very first Hackathon last week under the tutelage of my instructor Max Humber. This post serves to detail the inspirations behind the algorithm that underpins the Virality Prophet . My team was provided with YouTube datasets from US and Canada and given the task of producing an algorithm that would predict whether a YouTube video will go viral or not, based on user-inputs. Additionally this would serve as the backend with the frontend being hosted on Heroku. Click here for the github repo. EDA The following are the necessary packages. 1 2 3 4 5 6 7 8 9 10 11 12 import pandas as pd import numpy as np import re from sklearn.model_selection import train_test_split from sklearn_pandas import DataFrameMapper from sklearn.preprocessing import LabelBinarizer , StandardScaler from sklearn.feature_extraction.text import TfidfVectorizer from catboost import CatBoostClassifier , Pool from sklearn.pipeline import make_pipeline from sklearn.impute import SimpleImputer import pickle import seaborn as sns 1 2 3 4 5 6 7 8 # loading the datasets into DataFrames raw_df1 = pd . read_csv ( 'CAvideos.csv' ) raw_df2 = pd . read_csv ( 'USvideos.csv' ) # we decided to combine both datasets into one unified DataFrame raw_df = pd . concat ([ raw_df1 , raw_df2 ]) raw_df . reset_index ( inplace = True ) 1 raw_df . head () index video_id trending_date title channel_title category_id publish_time tags views likes dislikes comment_count thumbnail_link comments_disabled ratings_disabled video_error_or_removed description 0 0 n1WpP7iowLc 17.14.11 Eminem - Walk On Water (Audio) ft. BeyoncÃ© EminemVEVO 10 2017-11-10T17:00:03.000Z Eminem|\"Walk\"|\"On\"|\"Water\"|\"Aftermath/Shady/Inâ€¦ 17158579 787425 43420 125882 https://i.ytimg.com/vi/n1WpP7iowLc/default.jpg False False False Eminem's new track Walk on Water ft. BeyoncÃ© iâ€¦ 1 1 0dBIkQ4Mz1M 17.14.11 PLUSH - Bad Unboxing Fan Mail iDubbbzTV 23 2017-11-13T17:00:00.000Z plush|\"bad unboxing\"|\"unboxing\"|\"fan mail\"|\"idâ€¦ 1014651 127794 1688 13030 https://i.ytimg.com/vi/0dBIkQ4Mz1M/default.jpg False False False STill got a lot of packages. Probably will lasâ€¦ 2 2 5qpjK5DgCt4 17.14.11 Racist Superman | Rudy Mancuso, King Bach & Leâ€¦ Rudy Mancuso 23 2017-11-12T19:05:24.000Z racist superman|\"rudy\"|\"mancuso\"|\"king\"|\"bach\"â€¦ 3191434 146035 5339 8181 https://i.ytimg.com/vi/5qpjK5DgCt4/default.jpg False False False WATCH MY PREVIOUS VIDEO â–¶ \\n\\nSUBSCRIBE â–º httpâ€¦ 3 3 d380meD0W0M 17.14.11 I Dare You: GOING BALD !? nigahiga 24 2017-11-12T18:01:41.000Z ryan|\"higa\"|\"higatv\"|\"nigahiga\"|\"i dare you\"|\"â€¦ 2095828 132239 1989 17518 https://i.ytimg.com/vi/d380meD0W0M/default.jpg False False False I know it's been a while since we did this shoâ€¦ 4 4 2Vv-BfVoq4g 17.14.11 Ed Sheeran - Perfect (Official Music Video) Ed Sheeran 10 2017-11-09T11:04:14.000Z edsheeran|\"ed sheeran\"|\"acoustic\"|\"live\"|\"coveâ€¦ 33523622 1634130 21082 85067 https://i.ytimg.com/vi/2Vv-BfVoq4g/default.jpg False False False ðŸŽ§: https://ad.gt/yt-perfect\\nðŸ’°: https://atlantâ€¦ 1 2 # check the columns to see what we're given raw_df . columns 1 2 3 4 5 Index(['index', 'video_id', 'trending_date', 'title', 'channel_title', 'category_id', 'publish_time', 'tags', 'views', 'likes', 'dislikes', 'comment_count', 'thumbnail_link', 'comments_disabled', 'ratings_disabled', 'video_error_or_removed', 'description'], dtype='object') From the above, we can perform some additional feature engineering and target engineering. We will also drop some columns that serve no purpose to our analysis. First we'll extract and create the publish_date column from the publish_time column as shown below. 1 2 3 4 5 6 7 8 regex = r \"&#94;.*?(?=T)\" publish_date = [] for i in range ( len ( raw_df [ 'publish_time' ])): test_str = raw_df [ 'publish_time' ][ i ] matches = re . findall ( regex , test_str )[ 0 ] publish_date . append ( matches ) i += 1 raw_df [ 'publish_date' ] = publish_date Next we'll do some replacements on the trending_date column so that we can format it and publish_date as datetime . From there we created a days_to_trend column that reflects how many days a video took to trend. Note: \"trend\" is still subjective as a target but we'll get to that a bit later. 1 2 3 4 5 raw_df [ 'trending_date' ] = [ x . replace ( '.' , '-' ) for x in raw_df [ 'trending_date' ]] raw_df [ \"publish_date\" ] = pd . to_datetime ( raw_df [ \"publish_date\" ]) raw_df [ \"trending_date\" ] = pd . to_datetime ( raw_df [ \"trending_date\" ], format = '%y- %d -%m' ) raw_df [ \"days_to_trend\" ] = raw_df [ \"trending_date\" ] - raw_df [ \"publish_date\" ] raw_df [ \"days_to_trend\" ] = raw_df [ \"days_to_trend\" ] . apply ( lambda x : x . days ) 1 2 3 # convert the \"publish_date\" column to be a numeric category, # with 0 mapped to Monday and 6 mapped to Sunday raw_df [ \"publish_date\" ] = raw_df [ \"publish_date\" ] . apply ( lambda x : x . weekday ()) Target Engineering Recall that we still haven't dealt with what the \"target\" should be yet. From examination of the columns we're probably inclined to simply set the \"views\" column as the target and be done with it. However we ought to remember that we're tasked with predicting whether a video would go viral (or not) based on certain criteria. Thus the virality of a video should be predicated on a combination of views, likes and dislikes among other features; a combination of \"targets\" if you will. To simplify this, I came up with a unified \"virality\" metric based on 5 columns in the dataset: views likes dislikes comment_count days_to_trend We'll introduce a penalty term based on the days_to_trend where the goal is to \"reward\" those that took the fewer days to trend, while increasingly \"punishing\" those that took more days to trend. $$ \\text{days_penalty} = \\frac{1}{(w_{\\text{days}})*(1 + \\text{days_to_trend})}$$ To obtain the metric for each row, weights would be assigned to each of the other four targets and then combined with the days_penalty term as per the following equation: $$ \\text{metric} = \\text{days_penalty} * [(\\text{views} * w_1) + (\\text{likes}*w_2) + (\\text{dislikes}*w_3)+(\\text{comment_count}*w_4)] $$ As for setting the weight terms, these can be arbitrary depending on what we think the importance of each target should be. For our purposes I think \\(w_1\\) for views is the most important, followed by \\(w_2\\) , \\(w_4\\) and \\(w_3\\) . We'll also set the weight for days_penalty to be \\(0.13\\) to achieve the aforementioned effect. $$ w_1 = 0.5$$ $$ w_2 = 0.25$$ $$ w_3 = 0.1$$ $$ w_4 = 0.15$$ $$ w_\\text{days} = 0.13$$ 1 2 3 4 5 6 7 8 9 10 # create a unified metric based on 5 categories view_weight = 0.5 likes_weight = 0.25 dislikes_weight = 0.1 comment_weight = 0.15 days_weight = 0.13 raw_df [ \"days_penalty\" ] = 1 / ( days_weight * ( 1 + raw_df [ \"days_to_trend\" ])) raw_df [ \"metric\" ] = raw_df [ \"days_penalty\" ] * ( raw_df [ \"views\" ] * view_weight ) + ( raw_df [ \"likes\" ] * likes_weight ) + ( raw_df [ \"dislikes\" ] * dislikes_weight ) + ( raw_df [ \"comment_count\" ] * comment_weight ) raw_df [[ \"metric\" ]] . head () metric 0 1.341899e+07 1 1.985324e+06 2 4.129852e+06 3 2.722845e+06 4 2.191290e+07 Transforming Into a Classification Problem Following the steps above, we then decided to turn this problem into a classification one from a regression one. To do so, we'll have to set a cutoff point. Using the aforementioned equations in the Target Engineering section, we can calculate this cutoff point. Any data points whose metric falls below this cutoff will be assigned \\(0\\) while the opposite will be assigned a \\(1\\) in a new viral column. For our purposes we'll set the following benchmarks for each category. $$ \\text{views}: 1000000$$ $$ \\text{likes}: mean_\\text{likes}(\\text{views} > 1000000)$$ $$ \\text{dislikes}: mean_\\text{dislikes}(\\text{views} > 1000000)$$ $$ \\text{comment_count}: mean_\\text{comment_count}(\\text{views} > 1000000)$$ $$ \\text{days_to_trend}: 5$$ 1 2 3 4 5 6 view_cut = 1000000 # set cutoff at 1 million views day_cut = 5 # set cutoff days to trend likes_cut = raw_df [ \"likes\" ][ raw_df [ \"views\" ] > view_cut ] . mean () dislikes_cut = raw_df [ \"dislikes\" ][ raw_df [ \"views\" ] > view_cut ] . mean () comment_cut = raw_df [ \"comment_count\" ][ raw_df [ \"views\" ] > view_cut ] . mean () cutoff = np . log (( 1 / ( days_weight * ( 1 + day_cut ))) * (( view_cut * view_weight ) + ( likes_cut * likes_weight ) + ( dislikes_cut * dislikes_weight ) + ( comment_cut * comment_weight ))) Finally we'll log-transform our new metric column to make it better approximate a normal distribution, which will also positively affect our results. 1 2 3 raw_df [[ \"metric\" ]] = np . log ( raw_df [ \"metric\" ]) # log transform to approximate normal distribution raw_df [ \"viral\" ] = np . where ( raw_df [ \"metric\" ] >= cutoff , 1 , 0 ) # create new class column as target to predict virality 1 2 # plot log-transformed unified metric sns . distplot ( raw_df [ \"metric\" ]); Before we go further, there is one more feature engineering to do. From the length of the video title we can yet extract one more column, namely the title_len column. From a quick search we can find that the consensus for an optimal title length is between \\(60-70\\) characters. Note: by default YouTube allows a maximum of 100 characters in the title section. 1 2 3 # extract extra column from title length raw_df [ \"title_len\" ] = raw_df [ \"title\" ] . apply ( lambda x : len ( x )) raw_df [[ \"title_len\" ]] . head () title_len 0 42 1 29 2 53 3 24 4 43 Other EDA We're interested in using the TFIDF algorithm to extract the relevant keywords from our dataset. Instead of running an instance for each text column ( title , tags , description ), we're actually better served by combining all three columns into one and then running a single instance of TFIDF on it. 1 2 # fill NaN in \"description\" to run tfidf raw_df [ \"description\" ] . fillna ( \"\" , inplace = True ) 1 2 # combining all three text columns into one single column raw_df [ \"titles\" ] = raw_df [ \"title\" ] + \" \" + raw_df [ \"tags\" ] + \" \" + raw_df [ \"description\" ] We're now ready to perform our analysis. The irrelevant columns will be dropped first. The \"target\" is now the viral binary column, while the remaining feature columns are: publish_date category_id title_len titles To find out what each of the category_id maps to, we'll have to dig into the .json files provided. We also have to ensure that the category_id sets pulled from both the US and Canadian sets are the exact same. 1 2 3 4 cats_us = len ( pd . DataFrame ( pd . read_json ( \"US_category_id.json\" )[ \"items\" ])) categories_us = { pd . DataFrame ( pd . read_json ( \"US_category_id.json\" )[ \"items\" ]) . loc [ i ][ \"items\" ][ \"id\" ]: pd . DataFrame ( pd . read_json ( \"US_category_id.json\" )[ \"items\" ]) . loc [ i ][ \"items\" ][ \"snippet\" ][ \"title\" ] for i in range ( cats_us )} cats_ca = len ( pd . DataFrame ( pd . read_json ( \"CA_category_id.json\" )[ \"items\" ])) categories_ca = { pd . DataFrame ( pd . read_json ( \"CA_category_id.json\" )[ \"items\" ]) . loc [ i ][ \"items\" ][ \"id\" ]: pd . DataFrame ( pd . read_json ( \"CA_category_id.json\" )[ \"items\" ]) . loc [ i ][ \"items\" ][ \"snippet\" ][ \"title\" ] for i in range ( cats_ca )} Check if there's any difference between the two dictionary sets. 1 set ( categories_us . keys ()) - set ( categories_ca . keys ()) 1 {'29'} So the Canadian dictionary set is lacking the category_id: 29 . We'll stick to using the American dictionary set. 1 2 3 4 5 6 7 8 raw_df = raw_df . drop ([ 'index' , 'video_id' , 'channel_title' , 'publish_time' , 'views' , 'likes' , 'dislikes' , 'comment_count' , \"description\" , \"trending_date\" , \"title\" , \"tags\" , 'thumbnail_link' , 'comments_disabled' , 'ratings_disabled' , 'days_to_trend' , \"days_penalty\" , \"trending_date\" , 'video_error_or_removed' , \"metric\" ], axis = 1 ) target = 'viral' X = raw_df . drop ( target , axis = 1 ) y = raw_df [[ target ]] Perform our usual train_test_split . We'll then call a DataFrameMapper to funnel our columns into a transformation. 1 X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.3 , random_state = 42 ) 1 2 3 4 5 6 mapper = DataFrameMapper ([ ([ 'publish_date' ], [ SimpleImputer (), LabelBinarizer ()]), ([ 'category_id' ], [ SimpleImputer (), LabelBinarizer ()]), ([ 'title_len' ], [ SimpleImputer (), StandardScaler ()]), ( 'titles' , TfidfVectorizer ( stop_words = 'english' , max_features = 800 , token_pattern = u '(?ui) \\\\ b \\\\ w*[a-z]+ \\\\ w* \\\\ b' )) ], df_out = True ) Modelling with CatBoost Classifier With the mapper created, we can now use the CatBoost Classifier to fit our prediction algorithm. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # cat_features = [] Z_train = mapper . fit_transform ( X_train ) Z_test = mapper . transform ( X_test ) cat = CatBoostClassifier ( iterations = 5000 , eval_metric = \"F1\" , random_seed = 42 , learning_rate = 0.5 , early_stopping_rounds = 3000 ) train_pool = Pool ( data = Z_train , label = y_train ) validation_pool = Pool ( data = Z_test , label = y_test ) cat . fit ( train_pool , eval_set = validation_pool , verbose = False ) print ( f 'Model is fitted: { cat . is_fitted () } ' ) print ( f 'Model params: { cat . get_params () } ' ) print ( cat . best_score_ ) 1 2 3 Model is fitted: True Model params: {'iterations': 5000, 'learning_rate': 0.5, 'random_seed': 42, 'eval_metric': 'F1', 'early_stopping_rounds': 3000} {'learn': {'Logloss': 0.12699710020912125, 'F1': 0.9446652399877714}, 'validation': {'Logloss': 0.2595652853013616, 'F1': 0.8756729559748426}} Using F1 as the metric, the CatBoost Classifier was able to achieve a training set score of \\(94\\text{%}\\) with a test set score of \\(87\\text{%}\\) ! While there's some slight overfitting, for our purposes this is sufficiently decent to use as our virality predictor model. Wrapping Up I hope this post can serve as an inspiration for future readers to be more creative with their data (within good reason of course!). While the actions I had performed for both feature and target engineering was beyond what I've been exposed to at the time, it still felt right and logical and the results were able to speak for themselves. \" Data Science is sometimes more art than science.\" Max Humber, 2019","tags":"blog","url":"blog/2019/10/21/dsi-5-hackathon-the-youtube-virality-prophet/","loc":"blog/2019/10/21/dsi-5-hackathon-the-youtube-virality-prophet/"},{"title":"Multinomial & Dirichlet Distribution with PyMC3","text":"In this post, we'll explore the Multinomial and Dirichlet distributions for die rolls using the PyMC3 package. To toss things up a bit I'll simulate a biased D20 (D20 is a 20-sided die from my days as a Magic: the Gathering player). Multinomial Distribution The multinomial distribution is a generalization of the binomial distribution. For our purposes it adequately models the probability of counts of each side for rolling a \\(k\\) -sided die \\(n\\) amount of times. $$k = \\textrm{number of sides of a die}$$ $$n = \\textrm{number of rolls/trials of the die}$$ For \\(n\\) amount of independent trials, each of which leads to a success for exactly one of \\(k\\) categories, with each category having a given fixed probability of success, the multinomial distribution gives the probability of any particular combination of numbers of successes for the various \\(k\\) categories. Note the following behaviour as \\(k\\) and \\(n\\) is changed: - when \\(k\\) is \\(2\\) and \\(n\\) is \\(1\\) , the multinomial distribution is the Bernoulli distribution - when \\(k\\) is \\(2\\) and \\(n\\) is bigger than \\(1\\) , it is the binomial distribution - when \\(k\\) is bigger than \\(2\\) and \\(n\\) is \\(1\\) , it is the categorical distribution Dirichlet Distribution The Dirichlet distribution is a collection of continuous multivariate probability distribution parameterized by a vector \\(\\alpha\\) of positive real numbers. It is also a multivariate generalization of the Beta distribution. For our purposes in Bayesian statistics, it is commonly used as a prior and is the the conjugate prior of both the categorical distribution and multinomial distribution. 1 2 3 import pymc3 as pm import numpy as np import seaborn as sns 1 2 3 4 /home/duryan/anaconda3/lib/python3.7/site-packages/theano/configdefaults.py:560: UserWarning: DeprecationWarning: there is no c++ compiler.This is deprecated and with Theano 0.11 a c++ compiler will be mandatory warnings.warn(\"DeprecationWarning: there is no c++ compiler.\" WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string. WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions. We'll make the simulated dice to be weighted towards rolling the number \\(15\\) . Then we'll use both distributions to find any irregularities as well as which number it is (both of which we already know the answer to!). 1 2 3 4 5 # simulate an uneven die np . random . seed ( 42 ) # setting the seed so that our results are replicable die = list ( range ( 1 , 21 )) + [ 15 ] def die_roll (): return sum ( np . random . choice ( die , 1 , replace = True )) 1 2 3 4 # perform 150 rolls with the uneven die rolls = 150 rolled = [ die_roll () for i in range ( rolls )] rolled_count = np . array ([ rolled . count ( n ) for n in set ( die )]) 1 2 3 4 5 6 # examine the distribution of results from 150 rolls k = len ( rolled_count ) p = 1 / k n = rolls sns . barplot ( x = np . arange ( 1 , k + 1 ), y = rolled_count ); From the above we can sort of tell that the die is already weighted towards 15, but we're assuming we don't know that yet. Let's use the Bayesian method to conclude this. 1 2 3 4 5 6 7 8 9 10 11 model = pm . Model () # instantiate a model with model : # instantiate a Dirichlet distribution with a uniform prior with dimension k: a = np . ones ( k ) theta = pm . Dirichlet ( \"theta\" , a = a ) bias = pm . Deterministic ( \"bias\" , theta [ 15 - 1 ] - p ) results = pm . Multinomial ( \"results\" , n = n , p = theta , observed = rolled_count ) Since theta[14] will hold the posterior probability of rolling a \\(15\\) we'll set a Deterministic object to record and compare this to the reference value \\(p = 1/20\\) . 1 2 with model : trace = pm . sample ( draws = 1000 , tune = 300 , chains = 2 ) 1 2 3 4 5 Auto - assigning NUTS sampler ... Initializing NUTS using jitter + adapt_diag ... Multiprocess sampling ( 2 chains in 2 jobs ) NUTS : [ theta ] Sampling 2 chains : 100 %| â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2600 / 2600 [ 01:21<00:00, 31.74draws/s ] 1 pm . traceplot ( trace ); All our drawn thetas above seem to behave within our selected conjugate distributions, which is what we wanted to see. 1 2 # Generate summary of posterior distributions and round to three decimal places pm . summary ( trace ) . round ( 3 ) mean sd mc_error hpd_2.5 hpd_97.5 n_eff Rhat theta__0 0.047 0.016 0.0 0.019 0.080 4638.434 1.0 theta__1 0.047 0.016 0.0 0.018 0.079 5681.180 1.0 theta__2 0.059 0.018 0.0 0.026 0.093 3994.628 1.0 theta__3 0.053 0.017 0.0 0.019 0.083 5280.989 1.0 theta__4 0.041 0.015 0.0 0.013 0.071 4669.649 1.0 theta__5 0.024 0.011 0.0 0.004 0.046 5043.157 1.0 theta__6 0.077 0.020 0.0 0.038 0.116 4303.150 1.0 theta__7 0.065 0.019 0.0 0.032 0.103 4753.146 1.0 theta__8 0.047 0.017 0.0 0.016 0.078 6134.952 1.0 theta__9 0.041 0.015 0.0 0.015 0.071 4457.968 1.0 theta__10 0.041 0.014 0.0 0.016 0.070 3803.688 1.0 theta__11 0.070 0.021 0.0 0.033 0.111 3596.630 1.0 theta__12 0.047 0.016 0.0 0.019 0.082 4235.920 1.0 theta__13 0.041 0.015 0.0 0.014 0.070 4579.560 1.0 theta__14 0.130 0.026 0.0 0.081 0.181 4554.103 1.0 theta__15 0.029 0.013 0.0 0.007 0.053 5664.608 1.0 theta__16 0.030 0.013 0.0 0.006 0.054 4250.061 1.0 theta__17 0.035 0.015 0.0 0.009 0.062 3645.063 1.0 theta__18 0.041 0.015 0.0 0.014 0.071 4053.963 1.0 theta__19 0.035 0.014 0.0 0.009 0.062 4361.858 1.0 bias 0.080 0.026 0.0 0.031 0.131 4554.103 1.0 By examining the summary of our trace results, we can also clearly see that theta_14 which is number \\(15\\) , has a distinctly abnormal mean. 1 2 3 axes = pm . plot_posterior ( trace , varnames = [ \"theta\" ], ref_val = np . round ( p , 3 )) for i , ax in enumerate ( axes ): ax . set_title ( f \" { i + 1 } \" ) 1 2 / home / duryan / anaconda3 / lib / python3 .7 / site - packages / pymc3 / plots / __init__ . py : 40 : UserWarning : Keyword argument `varnames` renamed to `var_names` , and will be removed in pymc3 3.8 warnings . warn ( 'Keyword argument `{old}` renamed to `{new}`, and will be removed in pymc3 3.8' . format ( old = old , new = new )) In the posterior plot above, we can also see clearly that the 15th plot has a distinctly different outcome. Using our stored bias object, we can now calculate the probability that the number \\(15\\) is disproportionately rolled compared to the other numbers. 1 2 3 # calculating the bias bias_perc = len ( trace [ \"bias\" ][ trace [ \"bias\" ] > 0 ]) / len ( trace [ \"bias\" ]) print ( f \"The probability that number 15 is biased is { bias_perc : .2% } \" ) 1 The probability that number 15 is biased is 100.00%","tags":"blog","url":"blog/2019/10/05/multinomial-dirichlet-distribution-with-pymc3/","loc":"blog/2019/10/05/multinomial-dirichlet-distribution-with-pymc3/"},{"title":"Iterative Imputer","text":"The IterativeImputer is an estimator that's still experimental as of the time of writing (27 September 2019). The docstring reads: Multivariate imputer that estimates each feature from all the others. A strategy for imputing missing values by modeling each feature with missing values as a function of other features in a round-robin fashion. This means that the algorithm will take any feature with missing values and treat it like a pseudo-target, in turn using the other columns as its' features. The order of selected columns can be altered via its' parameters. Therefore its important to note that this process would introduce some measure of correlation to the data. Because the algorithm is using a regressor as an estimator to fill the columns with missing values, we will also want to ensure that each feature is as normally distributed as possible. This means that transformations may need to be performed to ensure optimal results. The Important Parameters estimator: default=BayesianRidge() The estimator to use at each step of the round-robin imputation. If sample_posterior is True, the estimator must support return_std in its predict method. Uses BayesianRidge by default. Can be swapped out for any other regressor such as KNeighborsRegressor or DecisionTreesRegressor among others. Make sure to define the parameters of the regressor when calling it as an estimator for the algorithm. missing_values : int, np.nan, optional (default=np.nan) The placeholder for the missing values. All occurrences of missing_values will be imputed. Use this to inform the algorithm as to how the missing values are defined in the dataset. sample_posterior : boolean, default=False Whether to sample from the (Gaussian) predictive posterior of the fitted estimator for each imputation. Estimator must support return_std in its predict method if set to True. Set to True if using IterativeImputer for multiple imputations. This parameter is usually usable only when a Bayesian regressor is used as an estimator. imputation_order : str, optional (default=\"ascending\") The order in which the features will be imputed. Possible values: - \"ascending\" - From features with fewest missing values to most. - \"descending\" - From features with most missing values to fewest. - \"roman\" - Left to right. - \"arabic\" - Right to left. - \"random\" - A random order for each round. Use this option to alter the order for which features get imputed first, as the round-robin nature of the algorithm means different results depending on the setting of this parameter. Overview This blog shall serve as a proof of concept to demonstrate the usage of the IterativeImputer as a viable strategy for filling in missing values within any feature columns. We will follow this procedure to test out the IterativeImputer : 1) Create a regression set. 2) Insert NaNs into regression set's features. 3) Impute NaNs with IterativeImputer first, followed by the SimpleImputer . IterativeImputer will go through the KNeighborsRegressor , LinearRegression and BayesianRidge estimators. 4) Fit the regression set to an ExtraTreesRegressor . 5) Predict the target using the IterativeImputer and SimpleImputer sets and compare MSE scores. 1) Make a Regression Set 1 2 3 4 5 6 7 8 9 10 # the import packages we'll need from sklearn.experimental import enable_iterative_imputer # this step is necessary because the IterativeImputer is still experimental from sklearn.impute import IterativeImputer , SimpleImputer from sklearn.datasets import make_regression import numpy as np import pandas as pd from sklearn.ensemble import ExtraTreesRegressor from sklearn.neighbors import KNeighborsRegressor from sklearn.linear_model import LinearRegression , BayesianRidge from sklearn.metrics import mean_squared_error 1 X , y , coef = make_regression ( n_samples = 100 , n_features = 5 , noise = 0.1 , coef = True , random_state = 42 ) 1 X = pd . DataFrame ( X , columns = [ \"feature_1\" , \"feature_2\" , \"feature_3\" , \"feature_4\" , \"feature_5\" ]) 2) Insert NaNs 1 2 3 4 5 6 7 8 np . random . seed ( 42 ) XX = X . copy () # fill a copy of the dataset with NaNs at random for i in np . random . choice (( range ( len ( XX . index ))), size = 20 , replace = False ): for col in np . random . choice ( XX . columns , size = 1 ): XX . loc [ i + np . random . choice ( range ( 10 )), col ] = np . NaN XX . loc [ i + np . random . choice ( range ( 10 )), col ] = np . NaN 3) Impute NaNs with IterativeImputer and SimpleImputer 1 2 3 4 # instantiate an instance of the IterativeImputer with the KNeighborsRegressor as an estimator ii_kn = IterativeImputer ( estimator = KNeighborsRegressor ( n_neighbors = 15 ), verbose = 2 , max_iter = 1000 ) # Set verbose=2 to see the workflow of the IterativeImputer XX_ii_kn = ii_kn . fit_transform ( XX ) 1 2 3 4 5 6 [ IterativeImputer ] Completing matrix with shape ( 100 , 5 ) [ IterativeImputer ] Ending imputation round 1 / 1000 , elapsed time 0.01 [ IterativeImputer ] Ending imputation round 2 / 1000 , elapsed time 0.02 [ IterativeImputer ] Ending imputation round 3 / 1000 , elapsed time 0.03 [ IterativeImputer ] Ending imputation round 4 / 1000 , elapsed time 0.03 [ IterativeImputer ] Early stopping criterion reached . 1 2 3 4 # instantiate an instance of the IterativeImputer with the LinearRegression as an estimator ii_lin = IterativeImputer ( estimator = LinearRegression (), verbose = 2 ) # Set verbose=2 to see the workflow of the IterativeImputer XX_ii_lin = ii_lin . fit_transform ( XX ) 1 2 3 4 5 [ IterativeImputer ] Completing matrix with shape ( 100 , 5 ) [ IterativeImputer ] Ending imputation round 1 / 10 , elapsed time 0.04 [ IterativeImputer ] Ending imputation round 2 / 10 , elapsed time 0.04 [ IterativeImputer ] Ending imputation round 3 / 10 , elapsed time 0.05 [ IterativeImputer ] Early stopping criterion reached . 1 2 3 4 # instantiate an instance of the IterativeImputer with the BayesianRidge as an estimator ii_br = IterativeImputer ( estimator = BayesianRidge (), verbose = 2 ) # Set verbose=2 to see the workflow of the IterativeImputer XX_ii_br = ii_br . fit_transform ( XX ) 1 2 3 4 [ IterativeImputer ] Completing matrix with shape ( 100 , 5 ) [ IterativeImputer ] Ending imputation round 1 / 10 , elapsed time 0.01 [ IterativeImputer ] Ending imputation round 2 / 10 , elapsed time 0.03 [ IterativeImputer ] Early stopping criterion reached . 1 2 si = SimpleImputer () XX_si = si . fit_transform ( XX ) 4) Fit to a ExtraTreesRegressor 1 2 et = ExtraTreesRegressor ( n_estimators = 100 , random_state = 42 ) et . fit ( X , y ) 1 2 3 4 5 6 7 ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None, oob_score=False, random_state=42, verbose=0, warm_start=False) 5) Predicting the Targets and Scoring 1 2 yhat = et . predict ( X ) print ( f \"MSE for base generated model: { mean_squared_error ( y , yhat ) } \" ) 1 MSE for base generated model: 4.78746936230924e-26 1 2 3 yhat_ii_lin = et . predict ( XX_ii_lin ) mean_squared_error ( y , yhat_ii_lin ) print ( f \"MSE for IterativeImputer with LinearRegression estimator: { mean_squared_error ( y , yhat_ii_lin ) } \" ) 1 MSE for IterativeImputer with LinearRegression estimator: 1060.1077381402583 1 2 3 yhat_ii_kn = et . predict ( XX_ii_kn ) mean_squared_error ( y , yhat_ii_kn ) print ( f \"MSE for IterativeImputer with KNeighborsRegressor estimator: { mean_squared_error ( y , yhat_ii_kn ) } \" ) 1 MSE for IterativeImputer with KNeighborsRegressor estimator: 855.3049715708974 1 2 3 yhat_ii_br = et . predict ( XX_ii_br ) mean_squared_error ( y , yhat_ii_br ) print ( f \"MSE for IterativeImputer with BayesianRidge estimator: { mean_squared_error ( y , yhat_ii_br ) } \" ) 1 MSE for IterativeImputer with BayesianRidge estimator: 1039.619040088303 1 2 3 yhat_si = et . predict ( XX_si ) mean_squared_error ( y , yhat_si ) print ( f \"MSE for SimpleImputer: { mean_squared_error ( y , yhat_si ) } \" ) 1 MSE for SimpleImputer: 1040.8413193768447 Conclusion As we can see from above, the Mean_Squared_Errors scores demonstate that the IterativeImputer yields superior results to the SimpleImputer. The single exception being when the simple LinearRegression was used as an estimator (let's just not use the basic LinearRegression at this point!). We should take note that the IterativeImputer is still in its' developmental stage and may still be subject to bugs and errors. Nevertheless the early indications are that it'll become another useful tool in any data scientist's arsenal to handle missing values.","tags":"blog","url":"blog/2019/09/30/iterative-imputer/","loc":"blog/2019/09/30/iterative-imputer/"},{"title":"Missing Indicator","text":"Overview This post will demonstrate the use of the MissingIndicator , showcasing the behaviour of the algorithm by altering parameters such as \"features\" and \"missing_values\" to make sense of the corresponding output. The docstring reads: Binary indicators for missing values. Note that this component typically should not be used in a vanilla Pipeline consisting of transformers and a classifier, but rather could be added using a FeatureUnion or ColumnTransformer The MissingIndicator class is typically used to transform a dataset into its corresponding binary matrix to help indicate the presence of missing values in the dataset. This transformation is useful in conjunction with (before) imputation. When using imputation, preserving prior information about which values had been missing can be informative. A NaN is the de facto placeholder for missing values and enforces the data type for that feature to be a float/object. However the parameter missing_values can allow the specification of other missing value placeholders such as integers i.e. \\(-1\\) to be identified by the algorithm and transformed. The Important Parameters missing_values : number, string, np.nan (default) or None The placeholder for the missing values. All occurrences of missing_values will be indicated (True in the output array), the other values will be marked as False. Specify the indicator for missing values in the dataset here if different from conventional NaN . features : str, optional Whether the imputer mask should represent all or a subset of features. - If \"missing-only\" (default), the imputer mask will only represent features containing missing values during fit time. - If \"all\", the imputer mask will represent all features. Change to \"all\" if the desired output has to retain all features of the dataset. Otherwise algorithm will only keep features that contain missing values. error_on_new : boolean, optional If True (default), transform will raise an error when there are features with missing values in transform that have no missing values in fit. This is applicable only when features=\"missing-only\" . Make sure the features with missing values are consistent at the fit and transform stages, otherwise this will output an error message. Parameters: missing_values = -1 & features = \"all\" With these parameter settings, we can expect an array of the same shape as the input to be output. The \" \\(-1\\) \" in the array will be set to True . 1 2 3 # the necessary imports for this exercise from sklearn.impute import MissingIndicator import numpy as np 1 2 3 4 x1 = np . array ([[ 9 , - 1 , 5 ], [ - 1 , 5 , 9 ], [ 7 , - 1 , - 1 ]]) x1 1 2 3 array([[ 9, -1, 5], [-1, 5, 9], [ 7, -1, -1]]) 1 2 3 4 # instantiate an instance of MissingIndicator with missing_values set to \"-1\" mi_x1 = MissingIndicator ( missing_values =- 1 , features = \"all\" ) x1_tr = mi_x1 . fit_transform ( x1 ) x1_tr 1 2 3 array([[False, True, False], [ True, False, False], [False, True, True]]) Parameters: missing_values = np.nan & features = \"all\" With these parameter settings, we can expect an array of the same shape as the input to be output. The \"np.nan\" in the array will be set to True . 1 2 3 4 x2 = np . array ([[ 9 , np . nan , 5 ], [ np . nan , 5 , np . nan ], [ 7 , - 1 , - 1 ]]) x2 1 2 3 array([[ 9., nan, 5.], [nan, 5., nan], [ 7., -1., -1.]]) 1 2 3 4 # instantiate an instance of MissingIndicator with missing_values set to \"-1\" mi_x2 = MissingIndicator ( missing_values = np . nan , features = \"all\" ) x2_tr = mi_x2 . fit_transform ( x2 ) x2_tr 1 2 3 array([[False, True, False], [ True, False, True], [False, False, False]]) Parameters: missing_values = np.nan & features = \"missing-only\" With these parameter settings, we expect an array of a different shape to the input to be output if there are no missing values on some features. The \"np.nan\" in the array will be set to True . In this case there will be no missing values in the second feature and only missing values on the second row of first and third columns. We should see an array with the remaining 2 columns transformed as a result. 1 2 3 4 x3 = np . array ([[ 9 , 20 , 5 ], [ np . nan , 5 , np . nan ], [ 7 , - 1 , - 1 ]]) x3 1 2 3 array([[ 9., 20., 5.], [nan, 5., nan], [ 7., -1., -1.]]) 1 2 x3_tr = indicator . transform ( x3 ) x3_tr 1 2 3 array([[False, False], [ True, True], [False, False]]) Conclusion The MissingIndicator while easy to implement, typically results in biased estimates as the transformed data significantly loses interpretability due to the binary nature of the outcome. While it is decent for a classifier algorithm to perform baseline estimates on, it is generally recommended that other advanced methods or imputers be used to account for missing values to ensure better results.","tags":"blog","url":"blog/2019/09/30/missing-indicator/","loc":"blog/2019/09/30/missing-indicator/"},{"title":"Passive-Aggressive Regression","text":"Foreword Why I'm Kai, an aspiring ðŸ€ sports âš½ analyst/enthusiast, self-proclaimed meme specialist, and avid durian connoisseur. This will be my first post of many, and today I would like to discuss the Passive Aggressive Regression method. INTRODUCTION The Passive-Aggressive Regression technique was famously introduced by Crammer, Dekel et.al in 2006 . It is a variation of the Logistic Regression where it's used to produce efficient step-wise algorithms. Without diving too much into the technical and/or mathematical details, we're basically going to introduce an additional factor to the hinge loss function ( \\(L\\) ) as shown below: the \" \\(\\epsilon\\) -insensitive\" loss. Its role is to allow for control of tolerance of prediction errors while fitting a model for continuous data. $$ L = \\begin{cases} 0 \\ \\ \\ if \\ \\ \\ |y_i - \\hat y_i|-\\epsilon \\leq 0\\\\ |y_i - \\hat y_i|-\\epsilon \\ \\ \\ \\ \\ \\ otherwise \\end{cases}$$ What the hinge loss function above is saying is basically one or the other would happen depending on what \\(|y_i - \\hat y_i|-\\epsilon\\) evaluates to. In the following example, assuming \\(|y_i - \\hat y_i|-\\epsilon\\) evaluated to a positive integer: $$|y_i - \\hat y_i|-\\epsilon > 0$$ $$ \\therefore L = |y_i - \\hat y_i|-\\epsilon$$ The hinge loss function \\(L\\) will take what \\(|y_i - \\hat y_i|-\\epsilon\\) evaluated to and force an adjustment. It is in essence the \"aggressive\" side of the regressor. However if \\(|y_i - \\hat y_i|-\\epsilon\\) evaluated to a value that was zero or lower, then: $$|y_i - \\hat y_i|-\\epsilon \\leq 0$$ $$ \\therefore L = 0$$ The loss function is forced to be zero, thereby providing the \"passive\" part of the regressor. To summarize, the \\(\\epsilon\\) value allows us to control how aggressive we want the loss hinge function to affect our model. Processing The Data For this exercise we'll utilize the Iris dataset available in the scikit-learn package. We'll perform a Multinomial Logistic Regression (more than 2 classes) and compare its results with that derived from the Passive-Aggressive Regression . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # These will be our imports for this example. from matplotlib import pyplot as plt import numpy as np import pandas as pd from scipy import stats from sklearn.linear_model import LogisticRegression , PassiveAggressiveRegressor from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.datasets import load_iris % config InlineBackend . figure_format = 'retina' % matplotlib inline The Iris dataset contains 150 rows of data and 4 feature columns, with only 3 unique, non-negimgative outcomes ( Classes ) when it comes to the dependent variable. 1 2 3 4 iris = load_iris () X = iris . data y = iris . target 1 2 print ( \"Unique class labels:\" , np . unique ( y )) print ( list ( iris . target_names )) 1 2 Unique class labels: [0 1 2] ['setosa', 'versicolor', 'virginica'] The class labels above, 0, 1, 2 have already been respectively assigned in place of each class of Iris flower: Iris-setosa \" , \" Iris-versicolor \" and \" Iris-virginica \" . Now that we have the independent and dependent datasets defined, let's perform a 30% test split with scikit-learn's train_test_split function. We'll also fix the random_state seed at 1990 to ensure our results are reproducible. 1 X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size =. 3 , random_state = 1990 , shuffle = True ) Note that we always want to perform a train_test_split before doing anything else with our data. Since we're interested in a Multinomial Logistic Regression for this exercise to compare with the results from a Passive-Aggressive Regression , we have to be cognizant of the fact that it only works for models with classified unique outcomes. Note: Multinomial Logistic Regression will be hereafter known as \"MLR\" . Passive-Aggressive Regression is hereafter known as \"PAR\" . Recall that the flowers have already been classified into 0 , 1 and 2 respectively! We can perform a MLR on this model and see how it performs compared to a PAR . Since we've already split our data, we're now ready to standardize our independent variables with the StandardScaler function found in scikit-learn's preprocessing package. 1 2 3 4 5 6 7 8 9 10 11 ss = StandardScaler () # We initiate a new instance of the StandardScaler class and assigned it to the variable \"ss\" ss . fit ( X_train ) # Using the fit method above, we obtain the sample mean and sample std.dev (scaling parameters) for each feature column from the training dataset. # The scaling parameters are stored specifically to the \"ss\" variable for the next step. Z_train = ss . transform ( X_train ) Z_test = ss . transform ( X_test ) # This step uses each of the feature's mean and std.dev and performs standardization to them. # We're using the same scaling parameters to transform the test dataset so that they are comparable to each other. # We're assigning each of the standardized features to new variables. The Multinomial Logistic Regression 1 2 3 4 lrg = LogisticRegression ( multi_class = \"multinomial\" , solver = \"newton-cg\" ) lrg . fit ( X_train , y_train ) print ( f \"The train set score is { lrg . score ( X_train , y_train ) } .\" ) print ( f \"The test set score is { lrg . score ( X_test , y_test ) } .\" ) 1 2 The train set score is 0.9714285714285714. The test set score is 0.9777777777777777. Note: I would include several Decision Surface plots via the Tom Dupre La Tour method in a future update. As we've discovered preliminarily, the MLR performed extremely well on the Iris dataset. Let's examine how the PAR performs. The Passive-Aggressive Regression 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 par = PassiveAggressiveRegressor ( C = 0.05 , loss = \"squared_epsilon_insensitive\" , epsilon = 0.001 , max_iter = 2000 , random_state = 1000 ) def par_train ( Ztrain , ytrain ): squared_errors = [] for ( x , y ) in zip ( Ztrain , ytrain ): par . partial_fit ( x . reshape ( 1 , - 1 ), y . ravel ()) y_pred = par . predict ( x . reshape ( 1 , - 1 )) squared_errors . append ( np . power ( y_pred - y , 2 )) sample = [ i for i in range ( 0 , len ( ytrain ))] print ( f \"The train set score is { par . score ( Ztrain , ytrain ) } .\" ) plt . figure ( figsize = ( 12 , 5 )) plt . plot ( sample , squared_errors ) plt . title ( \"Sample vs Squared Error (Training Set)\" ) plt . xlim ( 0 , len ( y_train ) + 10 ) plt . ylabel ( \"Squared Error\" ) plt . xlabel ( \"Sample\" ); 1 par_train ( Z_train , y_train ) 1 The train set score is 0.9056110143648486. 1 2 3 4 5 6 7 8 9 10 11 12 13 def par_test ( Ztest , ytest ): squared_errors = [] for ( x , y ) in zip ( Ztest , ytest ): y_pred = par . predict ( x . reshape ( 1 , - 1 )) squared_errors . append ( np . power ( y_pred - y , 2 )) sample = [ i for i in range ( 0 , len ( ytest ))] print ( f \"The test set score is { par . score ( Ztest , ytest ) } .\" ) plt . figure ( figsize = ( 12 , 5 )) plt . plot ( sample , squared_errors ) plt . title ( \"Sample vs Squared Error (Test Set)\" ) plt . xlim ( 0 , len ( ytest ) + 10 ) plt . ylabel ( \"Squared Error\" ) plt . xlabel ( \"Sample\" ); 1 par_test ( Z_test , y_test ) 1 The test set score is 0.9041007116772458. As we can see from the plots above, the model continuously attempted to bring the Loss function value ever closer to zero as more sample data was passed through it. Every new data point alters the existing trend, resulting in an oscillating behaviour. Whenever the model takes a sample whose result it fails to predict correctly, it gets penalized which results in the various spikes you see in these plots. The subsequent samples will force the model to readapt its parameters, but it's results will still reflect the effects of the previous samples. Therefore passive-aggressive algorithms tend to be conservative to previous knowledge. Once the model learns some form of dependency, its almost unable to forget it completely. This behaviour can be witnessed in the test plot, where it struggles to settle down at the latter samples, albeit with fewer samples than the training set. Conclusion While the PAR didn't fare as well as the MLR , the fact that the MLR overperformed with scores of 97% on both its train and test set predictions should warrant further examination to rule out overfitting. If a critical decision had to be made on the spot, I would err on the side of caution and select the model predicted by the PAR .","tags":"blog","url":"blog/2019/09/16/passive-aggressive-regression/","loc":"blog/2019/09/16/passive-aggressive-regression/"}]};