<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="stylesheet" href="../../../../../theme/css/style.min.css?fc5adb95">

        <link rel="dns-prefetch" href="//fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin>

        <meta name="google-site-verification" content="Bk4Z5ucHLyPXqlZlj5LzANpYBBSvxqBW4E8i-Kwf-bQ" />        <meta name="msvalidate.01" content="8FF1B025212A47B5B27CC47163A042F0" />        <meta name="author" content="Kai" />

        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content="machine learning, blog, " />

<meta property="og:title" content="Passive-Aggressive¬†Regression "/>
<meta property="og:url" content="../../../../../blog/2019/09/16/passive-aggressive-regression/" />
<meta property="og:description" content="Foreword¬∂ Why I‚Äôm Kai, an aspiring üèÄ sports ‚öΩ analyst/enthusiast, self-proclaimed meme specialist, and avid durian connoisseur. This will be my first post of many, and today I would like to discuss the Passive Aggressive Regression method.¬∂ INTRODUCTION¬∂ The Passive-Aggressive Regression technique was famously introduced by Crammer, Dekel et.al ‚Ä¶" />
<meta property="og:site_name" content="Data Super Saiyan" />
<meta property="og:article:author" content="Kai" />
<meta property="og:article:published_time" content="2019-09-16T00:30:00-04:00" />
<meta name="twitter:title" content="Passive-Aggressive¬†Regression ">
<meta name="twitter:description" content="Foreword¬∂ Why I‚Äôm Kai, an aspiring üèÄ sports ‚öΩ analyst/enthusiast, self-proclaimed meme specialist, and avid durian connoisseur. This will be my first post of many, and today I would like to discuss the Passive Aggressive Regression method.¬∂ INTRODUCTION¬∂ The Passive-Aggressive Regression technique was famously introduced by Crammer, Dekel et.al ‚Ä¶">

        <title>Passive-Aggressive¬†Regression  ¬∑ Data Super Saiyan
</title>
        <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Data Super Saiyan - Full Atom Feed" />
        <link href="/feeds/all.rss" type="application/rss+xml" rel="alternate" title="Data Super Saiyan - Full RSS Feed" />



    </head>
    <body>
        <div id="content">
            <div class="navbar navbar-static-top">
                <div class="navbar-inner">
                    <div class="container-fluid">
                        <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                        </a>
                        <a class="brand" href="../../../../../"><span class=site-name>Data Super Saiyan</span></a>
                        <div class="nav-collapse collapse">
                            <ul class="nav pull-right top-menu">
                                <li >
                                    <a href=
                                       ../../../../..
                                    >Home</a>
                                </li>
                                <li ><a href="../../../../../about/">About</a></li>
                                <li ><a href="../../../../../categories">Categories</a></li>
                                <li ><a href="../../../../../tags">Tags</a></li>
                                <li ><a href="../../../../../archives">Archives</a></li>
                                <li><form class="navbar-search" action="../../../../../search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="Search" name="q" id="tipue_search_input"></form></li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
            <div class="container-fluid">
                <div class="row-fluid">
                    <div class="span1"></div>
                    <div class="span10">
<article itemscope>
<div class="row-fluid">
    <header class="page-header span10 offset2">
        <h1>
            <a href="../../../../../blog/2019/09/16/passive-aggressive-regression/">
                Passive-Aggressive&nbsp;Regression
            </a>
        </h1>
    </header>
</div>

<div class="row-fluid">
        <div class="span8 offset2 article-content">
            
            <hr>
<h1 id="foreword">Foreword<a class="headerlink" href="#foreword" title="Permanent link">&para;</a></h1>
<h4 id="why-im-kai-an-aspiring-sports-analystenthusiast-self-proclaimed-meme-specialist-and-avid-durian-connoisseur-this-will-be-my-first-post-of-many-and-today-i-would-like-to-discuss-the-passive-aggressive-regression-method">Why <img src="https://media1.giphy.com/media/Nx0rz3jtxtEre/giphy.gif" width="200" height="100" /> I&#8217;m Kai, an aspiring üèÄ sports ‚öΩ analyst/enthusiast, self-proclaimed meme specialist, and avid durian connoisseur. This will be my first post of many, and today I would like to discuss the Passive Aggressive Regression method.<a class="headerlink" href="#why-im-kai-an-aspiring-sports-analystenthusiast-self-proclaimed-meme-specialist-and-avid-durian-connoisseur-this-will-be-my-first-post-of-many-and-today-i-would-like-to-discuss-the-passive-aggressive-regression-method" title="Permanent link">&para;</a></h4>
<h1 id="introduction"><strong><span class="caps">INTRODUCTION</span></strong><a class="headerlink" href="#introduction" title="Permanent link">&para;</a></h1>
<p>The <code>Passive-Aggressive Regression</code> technique was famously introduced by <a href="http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf">Crammer, Dekel et.al in 2006</a>. It is a variation of the Logistic Regression where it&#8217;s used to produce efficient step-wise&nbsp;algorithms.</p>
<p>Without diving too much into the technical and/or mathematical details, we&#8217;re basically going to introduce an additional factor to the hinge loss function (<span class="math">\(L\)</span>) as shown below: the &#8220;<span class="math">\(\epsilon\)</span>-insensitive&#8221; loss. Its role is to allow for control of tolerance of prediction errors while fitting a model for continuous&nbsp;data.</p>
<div class="math">$$ L = \begin{cases} 0 \ \ \ if \ \ \ |y_i - \hat y_i|-\epsilon \leq 0\\ |y_i - \hat y_i|-\epsilon \ \ \ \ \ \ otherwise \end{cases}$$</div>
<p>What the hinge loss function above is saying is basically one or the other would happen depending on what <span class="math">\(|y_i - \hat y_i|-\epsilon\)</span> evaluates&nbsp;to.</p>
<p>In the following example, assuming <span class="math">\(|y_i - \hat y_i|-\epsilon\)</span> evaluated to a positive&nbsp;integer:</p>
<div class="math">$$|y_i - \hat y_i|-\epsilon &gt; 0$$</div>
<div class="math">$$ \therefore L = |y_i - \hat y_i|-\epsilon$$</div>
<p>The hinge loss function <span class="math">\(L\)</span> will take what <span class="math">\(|y_i - \hat y_i|-\epsilon\)</span> evaluated to and force an adjustment. It is in essence the <code>"aggressive"</code> side of the&nbsp;regressor.</p>
<p>However if <span class="math">\(|y_i - \hat y_i|-\epsilon\)</span> evaluated to a value that was zero or lower,&nbsp;then:</p>
<div class="math">$$|y_i - \hat y_i|-\epsilon \leq 0$$</div>
<div class="math">$$ \therefore L = 0$$</div>
<p>The loss function is forced to be zero, thereby providing the <code>"passive"</code> part of the&nbsp;regressor.</p>
<p>To summarize, the <span class="math">\(\epsilon\)</span> value allows us to control how aggressive we want the loss hinge function to affect our&nbsp;model.</p>
<h1 id="processing-the-data">Processing The Data<a class="headerlink" href="#processing-the-data" title="Permanent link">&para;</a></h1>
<p>For this exercise we&#8217;ll utilize the <strong>Iris</strong> dataset available in the scikit-learn package. We&#8217;ll perform a <code>Multinomial Logistic Regression</code> (more than 2 classes) and compare its results with that derived from the <code>Passive-Aggressive Regression</code>.</p>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16</pre></div></td><td class="code"><div class="highlight"><pre><span></span><code><span class="c1"># These will be our imports for this example.</span>

<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">stats</span>

<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span><span class="p">,</span> <span class="n">PassiveAggressiveRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>

<span class="o">%</span><span class="n">config</span> <span class="n">InlineBackend</span><span class="o">.</span><span class="n">figure_format</span> <span class="o">=</span> <span class="s1">&#39;retina&#39;</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</code></pre></div>
</td></tr></table>
<p>The <strong>Iris</strong> dataset contains 150 rows of data and 4 feature columns, with only 3 unique, non-negimgative outcomes (<em>Classes</em>) when it comes to the dependent&nbsp;variable.</p>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3
4</pre></div></td><td class="code"><div class="highlight"><pre><span></span><code><span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>
</code></pre></div>
</td></tr></table>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2</pre></div></td><td class="code"><div class="highlight"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Unique class labels:&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">target_names</span><span class="p">))</span>
</code></pre></div>
</td></tr></table>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2</pre></div></td><td class="code"><div class="highlight"><pre><span></span><code><span class="err">Unique class labels: [0 1 2]</span>
<span class="err">[&#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39;]</span>
</code></pre></div>
</td></tr></table>
<p>The class labels above, <code>0, 1, 2</code> have already been respectively assigned in place of each class of <strong>Iris</strong> flower: <code>Iris-setosa</code><span class="dquo">&#8220;</span>, &#8220;<code>Iris-versicolor</code>&#8221; and &#8220;<code>Iris-virginica</code><span class="dquo">&#8220;</span>.</p>
<p>Now that we have the independent and dependent datasets defined, let&#8217;s perform a <code>30%</code> test split with scikit-learn&#8217;s <code>train_test_split</code> function. We&#8217;ll also fix the <code>random_state</code> seed at <code>1990</code> to ensure our results are&nbsp;reproducible.</p>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre>1</pre></div></td><td class="code"><div class="highlight"><pre><span></span><code><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=.</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1990</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>
</td></tr></table>
<p>Note that we always want to perform a <code>train_test_split</code> before doing anything else with our&nbsp;data.</p>
<p>Since we&#8217;re interested in a <code>Multinomial Logistic Regression</code> for this exercise to compare with the results from a <code>Passive-Aggressive Regression</code>, we have to be cognizant of the fact that it only works for models with classified unique&nbsp;outcomes.</p>
<p><em>Note: <code>Multinomial Logistic Regression</code> will be hereafter known as <code>"MLR"</code>. <code>Passive-Aggressive Regression</code> is hereafter known as <code>"PAR"</code>.</em></p>
<p>Recall that the flowers have already been classified into <code>0</code>, <code>1</code> and <code>2</code> respectively! We can perform a <code>MLR</code> on this model and see how it performs compared to a <code>PAR</code>.</p>
<p>Since we&#8217;ve already split our data, we&#8217;re now ready to standardize our independent variables with the <code>StandardScaler</code> function found in scikit-learn&#8217;s preprocessing&nbsp;package.</p>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11</pre></div></td><td class="code"><div class="highlight"><pre><span></span><code><span class="n">ss</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span> <span class="c1"># We initiate a new instance of the StandardScaler class and assigned it to the variable &quot;ss&quot;</span>

<span class="n">ss</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="c1"># Using the fit method above, we obtain the sample mean and sample std.dev (scaling parameters) for each feature column from the training dataset.</span>
<span class="c1"># The scaling parameters are stored specifically to the &quot;ss&quot; variable for the next step.</span>

<span class="n">Z_train</span> <span class="o">=</span> <span class="n">ss</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">Z_test</span> <span class="o">=</span> <span class="n">ss</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="c1"># This step uses each of the feature&#39;s mean and std.dev and performs standardization to them.</span>
<span class="c1"># We&#39;re using the same scaling parameters to transform the test dataset so that they are comparable to each other.</span>
<span class="c1"># We&#39;re assigning each of the standardized features to new variables.</span>
</code></pre></div>
</td></tr></table>
<h1 id="the-multinomial-logistic-regression"><strong>The Multinomial Logistic Regression</strong><a class="headerlink" href="#the-multinomial-logistic-regression" title="Permanent link">&para;</a></h1>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3
4</pre></div></td><td class="code"><div class="highlight"><pre><span></span><code><span class="n">lrg</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">multi_class</span><span class="o">=</span><span class="s2">&quot;multinomial&quot;</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s2">&quot;newton-cg&quot;</span><span class="p">)</span>
<span class="n">lrg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The train set score is </span><span class="si">{</span><span class="n">lrg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The test set score is </span><span class="si">{</span><span class="n">lrg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
</code></pre></div>
</td></tr></table>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2</pre></div></td><td class="code"><div class="highlight"><pre><span></span><code><span class="err">The train set score is 0.9714285714285714.</span>
<span class="err">The test set score is 0.9777777777777777.</span>
</code></pre></div>
</td></tr></table>
<h3 id="note-i-would-include-several-decision-surface-plots-via-the-tom-dupre-la-tour-method-in-a-future-update"><strong>Note: I would include several Decision Surface plots via the Tom Dupre La Tour method in a future update.</strong><a class="headerlink" href="#note-i-would-include-several-decision-surface-plots-via-the-tom-dupre-la-tour-method-in-a-future-update" title="Permanent link">&para;</a></h3>
<p>As we&#8217;ve discovered preliminarily, the <code>MLR</code> performed extremely well on the Iris dataset. Let&#8217;s examine how the <code>PAR</code> performs.</p>
<h1 id="the-passive-aggressive-regression"><strong>The Passive-Aggressive Regression</strong><a class="headerlink" href="#the-passive-aggressive-regression" title="Permanent link">&para;</a></h1>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16</pre></div></td><td class="code"><div class="highlight"><pre><span></span><code><span class="n">par</span> <span class="o">=</span> <span class="n">PassiveAggressiveRegressor</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s2">&quot;squared_epsilon_insensitive&quot;</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">par_train</span><span class="p">(</span><span class="n">Ztrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">):</span>
    <span class="n">squared_errors</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">Ztrain</span><span class="p">,</span><span class="n">ytrain</span><span class="p">):</span>
        <span class="n">par</span><span class="o">.</span><span class="n">partial_fit</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">par</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">squared_errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">sample</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">ytrain</span><span class="p">))]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The train set score is </span><span class="si">{</span><span class="n">par</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">Ztrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">squared_errors</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Sample vs Squared Error (Training Set)&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span><span class="o">+</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Squared Error&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Sample&quot;</span><span class="p">);</span>
</code></pre></div>
</td></tr></table>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre>1</pre></div></td><td class="code"><div class="highlight"><pre><span></span><code><span class="n">par_train</span><span class="p">(</span><span class="n">Z_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div>
</td></tr></table>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre>1</pre></div></td><td class="code"><div class="highlight"><pre><span></span><code><span class="err">The train set score is 0.9056110143648486.</span>
</code></pre></div>
</td></tr></table>
<p><img src="images/output_20_1.png" width="750" height="480" /></p>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13</pre></div></td><td class="code"><div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">par_test</span><span class="p">(</span><span class="n">Ztest</span><span class="p">,</span> <span class="n">ytest</span><span class="p">):</span>
    <span class="n">squared_errors</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">Ztest</span><span class="p">,</span><span class="n">ytest</span><span class="p">):</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">par</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">squared_errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">sample</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">ytest</span><span class="p">))]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The test set score is </span><span class="si">{</span><span class="n">par</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">Ztest</span><span class="p">,</span> <span class="n">ytest</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">squared_errors</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Sample vs Squared Error (Test Set)&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">ytest</span><span class="p">)</span><span class="o">+</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Squared Error&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Sample&quot;</span><span class="p">);</span>
</code></pre></div>
</td></tr></table>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre>1</pre></div></td><td class="code"><div class="highlight"><pre><span></span><code><span class="n">par_test</span><span class="p">(</span><span class="n">Z_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</code></pre></div>
</td></tr></table>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre>1</pre></div></td><td class="code"><div class="highlight"><pre><span></span><code><span class="err">The test set score is 0.9041007116772458.</span>
</code></pre></div>
</td></tr></table>
<p><img src="images/output_22_1.png" width="750" height="480" /></p>
<p>As we can see from the plots above, the model continuously attempted to bring the Loss function value ever closer to zero as more sample data was passed through it. Every new data point alters the existing trend, resulting in an oscillating&nbsp;behaviour.</p>
<p>Whenever the model takes a sample whose result it fails to predict correctly, it gets penalized which results in the various spikes you see in these plots. The subsequent samples will force the model to readapt its parameters, but it&#8217;s results will still reflect the effects of the previous&nbsp;samples.</p>
<p>Therefore passive-aggressive algorithms tend to be conservative to previous knowledge. Once the model learns some form of dependency, its almost unable to forget it completely. This behaviour can be witnessed in the test plot, where it struggles to settle down at the latter samples, albeit with fewer samples than the training&nbsp;set.</p>
<h1 id="conclusion">Conclusion<a class="headerlink" href="#conclusion" title="Permanent link">&para;</a></h1>
<p>While the <code>PAR</code> didn&#8217;t fare as well as the <code>MLR</code>, the fact that the <code>MLR</code> overperformed with scores of <code>97%</code> on both its train and test set predictions should warrant further examination to rule out&nbsp;overfitting.</p>
<p>If a critical decision had to be made on the spot, I would err on the side of caution and select the model predicted by the <code>PAR</code>.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>


             
 
                <p id="post-share-links">
    Like this post? Share on:
      <a href="https://twitter.com/intent/tweet?text=Passive-Aggressive%C2%A0Regression&url=/blog/2019/09/16/passive-aggressive-regression/&hashtags=machine-learning" target="_blank" rel="nofollow noopener noreferrer" title="Share on Twitter">Twitter</a>
 ‚ùÑ       <a href="https://www.facebook.com/sharer/sharer.php?u=/blog/2019/09/16/passive-aggressive-regression/" target="_blank" rel="nofollow noopener noreferrer" title="Share on Facebook">Facebook</a>
 ‚ùÑ       <a href="mailto:?subject=Passive-Aggressive%C2%A0Regression&amp;body=/blog/2019/09/16/passive-aggressive-regression/" target="_blank" rel="nofollow noopener noreferrer" title="Share via Email">Email</a>

            
            







            <hr/>
<section>
    <h2>Keep Reading</h2>
<ul class="related-posts-list">
<li><a href="../../../../../blog/2019/10/05/multinomial-dirichlet-distribution-with-pymc3/" title="Multinomial &amp; Dirichlet Distribution with¬†PyMC3">Multinomial & Dirichlet Distribution with¬†PyMC3</a></li>
<li><a href="../../../../../blog/2019/10/21/dsi-5-hackathon-the-youtube-virality-prophet/" title="DSI 5 Hackathon - The (YouTube) Virality¬†Prophet">DSI 5 Hackathon - The (YouTube) Virality¬†Prophet</a></li>
<li><a href="../../../../../blog/2019/11/12/point-forward-the-job-description-hacker/" title="Point Forward: The Job Description¬†Hacker">Point Forward: The Job Description¬†Hacker</a></li>
<li><a href="../../../../../blog/2019/12/01/how-to-run-your-nltk-pythonic-app-on-heroku/" title="How to Run Your NLTK Pythonic App on¬†Heroku">How to Run Your NLTK Pythonic App on¬†Heroku</a></li>
<li><a href="../../../../../blog/2020/02/20/takeaways-from-amazons-machine-learning-use-case-call-center/" title="Takeaways from Amazon‚Äôs Machine Learning Use Case: Call¬†Center">Takeaways from Amazon‚Äôs Machine Learning Use Case: Call¬†Center</a></li>
</ul>
<hr />
</section>
            <aside>
            <nav>
            <ul class="articles-timeline">
                <li class="next-article"><a href="../../../../../blog/2019/09/30/missing-indicator/" title="Next: Missing¬†Indicator">Missing¬†Indicator</a> ¬ª</li>
            </ul>
            </nav>
            </aside>
        </div>
        <section id="article-sidebar" class="span2">
    <h4>Reading Time</h4>
    <p>~5 min read</p>
            <h4>Published</h4>
            <time itemprop="dateCreated" datetime="2019-09-16T00:30:00-04:00">Mon 16 September 2019</time>
            <h4>Category</h4>
            <a class="category-link" href="../../../../../categories#blog-ref">blog</a>
            <h4>Tags</h4>
            <ul class="list-of-tags tags-in-article">
                <li><a href="../../../../../tags#machine-learning-ref">machine learning
                    <span class="superscript">8</span>
</a></li>
            </ul>
<h4>Stay in Touch</h4>
<div id="sidebar-social-link">
    <a href="http://twitter.com/iranzop" title="" target="_blank" rel="nofollow noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" aria-label="Twitter" role="img" viewBox="0 0 512 512"><rect width="512" height="512" rx="15%" fill="#1da1f3"/><path fill="#fff" d="M437 152a72 72 0 0 1-40 12 72 72 0 0 0 32-40 72 72 0 0 1-45 17 72 72 0 0 0-122 65 200 200 0 0 1-145-74 72 72 0 0 0 22 94 72 72 0 0 1-32-7 72 72 0 0 0 56 69 72 72 0 0 1-32 1 72 72 0 0 0 67 50 200 200 0 0 1-105 29 200 200 0 0 0 309-179 200 200 0 0 0 35-37"/></svg>
    </a>
    <a href="http://github.com/iranzo" title="" target="_blank" rel="nofollow noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" aria-label="GitHub" role="img" viewBox="0 0 512 512"><rect width="512" height="512" rx="15%" fill="#1B1817"/><path fill="#fff" d="M335 499c14 0 12 17 12 17H165s-2-17 12-17c13 0 16-6 16-12l-1-50c-71 16-86-28-86-28-12-30-28-37-28-37-24-16 1-16 1-16 26 2 40 26 40 26 22 39 59 28 74 22 2-17 9-28 16-35-57-6-116-28-116-126 0-28 10-51 26-69-3-6-11-32 3-67 0 0 21-7 70 26 42-12 86-12 128 0 49-33 70-26 70-26 14 35 6 61 3 67 16 18 26 41 26 69 0 98-60 120-117 126 10 8 18 24 18 48l-1 70c0 6 3 12 16 12z"/></svg>
    </a>
    <a href="https://www.linkedin.com/in/iranzo/" title="" target="_blank" rel="nofollow noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" aria-label="LinkedIn" role="img" viewBox="0 0 512 512" fill="#fff"><rect width="512" height="512" rx="15%" fill="#0077b5"/><circle cx="142" cy="138" r="37"/><path stroke="#fff" stroke-width="66" d="M244 194v198M142 194v198"/><path d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32"/></svg>
    </a>
</div>
            





            





        </section>
</div>
</article>
<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe.
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides.
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo https://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>                    </div>
                    <div class="span1"></div>
                </div>
            </div>
        </div>
<footer>

    <div>
        <span class="site-name">Data Super Saiyan</span> - Where we go even further beyond!
    </div>


    <span class="site-name">Data Super Saiyan</span>  is a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for sites to earn advertising fees by advertising and linking to Amazon.com.
    <div id="amzn-assoc-ad-23824450-ef77-4537-9259-8590465886f1">
    </div>
    <script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&adInstanceId=23824450-ef77-4537-9259-8590465886f1">
    </script>

    <div id="fpowered">
        Powered by: <a href="http://getpelican.com/" title="Pelican Home Page" target="_blank" rel="nofollow noopener noreferrer">Pelican</a>
        Theme: <a href="https://elegant.oncrashreboot.com/" title="Theme Elegant Home Page" target="_blank" rel="nofollow noopener noreferrer">Elegant</a>
    </div>
</footer>            <script src="//code.jquery.com/jquery.min.js"></script>
        <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/js/bootstrap.min.js"></script>
        <script src="../../../../../theme/js/elegant.prod.9e9d5ce754.js"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>

    <script>
    (function () {
        if (window.location.hash.match(/^#comment-\d+$/)) {
            $('#comment_thread').collapse('show');
        }
    })();
    window.onhashchange=function(){
        if (window.location.hash.match(/^#comment-\d+$/))
            window.location.reload(true);
    }
    $('#comment_thread').on('shown', function () {
        var link = document.getElementById('comment-accordion-toggle');
        var old_innerHTML = link.innerHTML;
        $(link).fadeOut(200, function() {
            $(this).text('Click here to hide comments').fadeIn(200);
        });
        $('#comment_thread').on('hidden', function () {
            $(link).fadeOut(200, function() {
                $(this).text(old_innerHTML).fadeIn(200);
            });
        })
    })
</script>

    </body>
    <!-- Theme: Elegant built for Pelican
        License : MIT -->
</html>